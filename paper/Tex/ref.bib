@manual{Arbi2019,
author = {Arbi, Riaz},
title = {{A Reproducible Event-based Backtester Written in R}},
url = {https://github.com/riazarbi/equity_analysis},
year = {2019}
}
@book{Loonat2018,
abstract = {We consider and extend the adversarial agent-based learning approach of Gy{\{}$\backslash$"o{\}}rfi {\{}$\backslash$it et al{\}} to the situation of zero-cost portfolio selection implemented with a quadratic approximation derived from the mutual fund separation theorems. The algorithm is applied to daily sampled sequential Open-High-Low-Close data and sequential intraday 5-minute bar-data from the Johannesburg Stock Exchange (JSE). Statistical tests of the algorithms are considered. The algorithms are directly compared to standard NYSE test cases from prior literature. The learning algorithm is used to select parameters for agents (or experts) generated by pattern matching past dynamics using a simple nearest-neighbour search algorithm. It is shown that there is a speed advantage associated with using an analytic solution of the mutual fund separation theorems. It is argued that the expected loss in performance does not undermine the potential application to intraday quantitative trading and that when transactions costs and slippage are considered the strategies can still remain profitable when unleveraged. The paper demonstrates that patterns in financial time-series on the JSE can be systematically exploited in collective but that this does not imply predictability of the individual asset time-series themselves.},
archivePrefix = {arXiv},
arxivId = {1605.04600},
author = {Loonat, Fayyaaz and Gebbie, Tim},
booktitle = {PLoS ONE},
doi = {10.1371/journal.pone.0202788},
eprint = {1605.04600},
file = {:home/riaz/Downloads/Learning zero-cost portfolio selection with Pattern Matching (Loonat Gebbie 2018).pdf:pdf},
isbn = {1111111111},
issn = {19326203},
number = {9},
pages = {1--38},
title = {{Learning zero-cost portfolio selection with pattern matching}},
volume = {13},
year = {2018}
}
@article{Wilcox2015a,
abstract = {We discuss the finding that cross-sectional characteristic based models  have yielded portfolios with higher excess monthly returns but lower risk than their arbitrage pricing theory  counterparts  in an analysis of equity returns of stocks listed on the JSE. Under the assumption of general no-arbitrage conditions, we argue that evidence in favour of characteristic based pricing implies that information is more likely assimilated  by means of nonlinear pricing kernels for the markets considered.},
archivePrefix = {arXiv},
arxivId = {1310.4067},
author = {Wilcox, Diane L. and Gebbie, Tim J.},
doi = {10.1080/10293523.2014.994437},
eprint = {1310.4067},
file = {:home/riaz/Downloads/Information Or Risk (Wilcox Gebbie 2015).pdf:pdf},
issn = {20770227},
journal = {Investment Analysts Journal},
keywords = {Arbitrage pricing theory,Characteristic-based models,Linear pricing kernel,Non-linear pricing kernel,Size effect,Value effect},
number = {1},
pages = {1--19},
title = {{On pricing kernels, information and risk}},
volume = {44},
year = {2015}
}
@article{Harvey2017,
abstract = {We report on the occurrence of an anomaly in the price impacts of small transaction volumes following a change in the fee structure of an electronic market. We first review evidence for the existence of a master curve for price impact on the Johannesburg Stock Exchange (JSE). On attempting to re-estimate a master curve after fee reductions, it is found that the price impact corresponding to smaller volume trades is greater than expected relative to prior estimates for a range of listed stocks. We show that a master curve for price impact can be found following rescaling by an appropriate liquidity proxy, providing a means for practitioners to approximate price impact curves without onerous processing of tick data.},
archivePrefix = {arXiv},
arxivId = {1602.04950},
author = {Harvey, M. and Hendricks, D. and Gebbie, T. and Wilcox, D.},
doi = {10.1016/j.physa.2016.11.042},
eprint = {1602.04950},
file = {:home/riaz/Downloads/Deviations in expected price impact for small transaction (Harvey Hendricks Gebbie and Wilcox 2017).pdf:pdf},
issn = {03784371},
journal = {Physica A: Statistical Mechanics and its Applications},
keywords = {Electronic limit order book,Fee structure change,Market microstructure,Market regulation,Master curve,Price impact},
pages = {416--426},
publisher = {Elsevier B.V.},
title = {{Deviations in expected price impact for small transaction volumes under fee restructuring}},
url = {http://dx.doi.org/10.1016/j.physa.2016.11.042},
volume = {471},
year = {2017}
}
@article{Flint2001,
author = {Flint, Emlyn and Seymour, Anthony},
file = {:home/riaz/Downloads/03{\_}factor-investing-in-sa{\_}9-14-17.pdf:pdf},
title = {{Factor Investing In South Africa}},
year = {2001}
}
@misc{Quantopian2018a,
author = {Quantopian},
title = {{Zipline: A Pythonic algorithmic trading library}},
url = {https://www.zipline.io/},
urldate = {2018-12-10},
year = {2018}
}
@manual{Peterson2018,
annote = {R package version 1.1.0},
author = {Peterson, Brian G and Carl, Peter},
title = {{PortfolioAnalytics: Portfolio Analysis, Including Numerical Methods for Optimization of Portfolios}},
url = {https://cran.r-project.org/package=PortfolioAnalytics},
year = {2018}
}
@article{Carr2015,
abstract = {Calibrating a trading rule using a historical simulation (also called backtest) contributes to backtest overfitting, which in turn leads to underperformance. In this paper we propose a procedure for determining the optimal trading rule (OTR) without running alternative model configurations through a backtest engine. We present empirical evidence of the existence of such optimal solutions for the case of prices following a discrete Ornstein-Uhlenbeck process, and show how they can be computed numerically. Although we do not derive a closed-form solution for the calculation of OTRs, we conjecture its existence on the basis of the empirical evidence presented.},
archivePrefix = {arXiv},
arxivId = {1408.1159},
author = {Carr, Peter and {Lopez de Prado}, Marcos},
doi = {10.2139/ssrn.2658641},
eprint = {1408.1159},
file = {:home/riaz/Downloads/SSRN-id2658641.pdf:pdf},
isbn = {0362-8140 (Print)$\backslash$r0362-8140 (Linking)},
issn = {1556-5068},
journal = {Ssrn},
keywords = {E44,G0,G1,G15,G2,G24,Trading,backtesting,optimization,overfitting,simulation},
number = {August},
pmid = {323680},
title = {{Determining Optimal Trading Rules Without Backtesting}},
year = {2015}
}
@article{Jarrow2012a,
abstract = {We improve upon the power of the statistical arbitrage test in Hogan, Jarrow, Teo, and Warachka (2004). Our methodology also allows for the evaluation of return anomalies under weaker assumptions. We then compare strategies based on their convergence rates to arbitrage and identify strategies whose probability of a loss declines to zero most rapidly. These strategies are preferred by investors with finite horizons or limited capital. After controlling for market frictions and examining convergence rates to arbitrage, we find that momentum and value strategies offer the most desirable trading opportunities. {\textcopyright} 2011 Elsevier B.V.},
author = {Jarrow, Robert and Teo, Melvyn and Tse, Yiu Kuen and Warachka, Mitch},
doi = {10.1016/j.finmar.2011.08.003},
file = {:home/riaz/Downloads/An Improved Test for Statistical Arbitrage.pdf:pdf},
issn = {13864181},
journal = {Journal of Financial Markets},
keywords = {Bootstrap,Momentum strategy,Statistical arbitrage,Value strategy},
number = {1},
pages = {47--80},
title = {{An improved test for statistical arbitrage}},
volume = {15},
year = {2012}
}
@manual{Carl2018,
annote = {R package version 0.14.6},
author = {Carl, Peter and Peterson, Brian G and Ulrich, Joshua and Humme, Jan},
title = {{quantstrat: Quantitative Strategy Model Framework}},
year = {2018}
}
@manual{Armstrong2018,
annote = {R package version 0.3.8},
author = {Armstrong, Whit and Eddelbuettel, Dirk and Laing, John},
title = {{Rblpapi: R Interface to 'Bloomberg'}},
url = {https://cran.r-project.org/package=Rblpapi},
year = {2018}
}
@inproceedings{Peterson2017a,
author = {Peterson, Brian G},
file = {:home/riaz/Downloads/STAC-Summit-17-Oct-2017-Peterson.pdf:pdf},
number = {October},
title = {{Machine Learning in Trading Things to Consider Introductions}},
year = {2017}
}
@article{Peterson2017,
abstract = {Analysts and portfolio managers face many challenges in developing new systematic trading systems. This paper provides a detailed, re- peatable process to aid in evaluating new ideas, developing those ideas into testable hypotheses, measuring results in comparable ways, and avoiding and measuring the ever-present risks of over-fittin},
archivePrefix = {arXiv},
arxivId = {1601.04920},
author = {Peterson, Brian G},
doi = {10.20964/2016.12.40},
eprint = {1601.04920},
file = {:home/riaz/Downloads/strat{\_}dev{\_}process.pdf:pdf},
isbn = {1581136625},
issn = {10495258},
number = {June},
pages = {42},
title = {{Developing {\&} Backtesting Systematic Trading Strategies}},
year = {2017}
}
@book{Graham1934a,
author = {Graham, B and Dodd, D and Dodd, D L F},
isbn = {9780070244962},
publisher = {McGraw-Hill Education},
title = {{Security Analysis: The Classic 1934 Edition}},
url = {https://books.google.co.za/books?id=wXlrnZ1uqK0C},
year = {1934}
}
@article{Brinkman,
author = {Brinkman, Richard G and Fellow, Faculty},
file = {:home/riaz/Downloads/Benjamin-Graham{\_}-David-Dodd-Security-Analysis-Sixth-Edition{\_}-Foreword-by-Warren-Buffett.pdf:pdf},
isbn = {0071425837},
title = {{7 0 + D V D ' s F O R S A L E {\&} E X C H A N G E}}
}
@article{VanRoy2009,
abstract = {This chapter gives an introduction to all the main programming paradigms, their underlying concepts, and the relationships between them. We give a broad view to help programmers choose the right concepts they need to solve the problems at hand. We give a taxonomy of almost 30 useful programming paradigms and how they are related. Most of them differ only in one or a few concepts, but this can make a world of difference in programming. We explain briefly how programming paradigms influence language design, and we show two sweet spots: dual-paradigm languages and a definitive language. We introduce the main concepts of programming languages: records, closures, independence (concurrency), and named state. We explain the main principles of data abstraction and how it lets us organize large programs. Finally, we conclude by focusing on concurrency, which is widely considered the hardest concept to program with. We present four little-known but important paradigms that greatly simplify concurrent programming with respect to mainstream languages: declarative concurrency (both eager and lazy), functional reactive programming, discrete synchronous programming, and constraint programming. These paradigms have no race conditions and can be used in cases where no other paradigm works. We explain why for multi-core processors and we give several examples from computer music, which often uses these paradigms.},
author = {{Van Roy}, P},
doi = {10.1016/j.tet.2003.02.005},
file = {:home/riaz/Downloads/VanRoyChapter.pdf:pdf},
isbn = {9782752100542},
journal = {New Computational Paradigms for Computer Music},
pages = {9--47},
title = {{Programming Paradigms for Dummies: What Every Programmer Should Know}},
url = {http://www.dmi.unict.it/{~}barba/PROG-LANG/PROGRAMMI-TESTI/READING-MATERIAL/VanRoyChapter.pdf},
year = {2009}
}
@book{Trice2017,
author = {Trice, Tim},
title = {{Backtesting Strategies with R}},
url = {https://timtrice.github.io/backtesting-strategies/index.html},
year = {2017}
}
@misc{Zipline2018,
author = {Zipline},
title = {{Zipline Beginner Tutorial — Zipline 1.3.0 documentation}},
url = {https://www.zipline.io/beginner-tutorial.html},
urldate = {2018-11-19},
year = {2018}
}
@misc{Quantopian2018,
author = {Quantopian},
title = {{Improved Backtest Analysis}},
url = {https://www.quantopian.com/posts/improved-backtest-analysis},
urldate = {2018-11-19},
year = {2018}
}
@misc{Quantpedia2018,
author = {Quantpedia},
title = {{Backtesting Software}},
url = {https://quantpedia.com/Links/Backtesters},
urldate = {2018-11-19},
year = {2018}
}
@book{Raymond:2003:AUP:829549,
author = {Raymond, Eric S},
isbn = {0131429019},
publisher = {Pearson Education},
title = {{The Art of UNIX Programming}},
year = {2003}
}
@book{Damodaran2012,
author = {Damodaran, A},
isbn = {9781118011515},
publisher = {Wiley},
series = {Wiley Finance Editions},
title = {{Investment Philosophies: Successful Strategies and the Investors Who Made Them Work}},
url = {https://books.google.co.za/books?id=NkOkN0l3vHgC},
year = {2012}
}
@article{Campbell1998,
author = {Campbell, John Y and Shiller, Robert J},
doi = {10.3905/jpm.24.2.11},
issn = {0095-4918},
journal = {The Journal of Portfolio Management},
number = {2},
pages = {11--26},
publisher = {Institutional Investor Journals Umbrella},
title = {{Valuation Ratios and the Long-Run Stock Market Outlook}},
url = {http://jpm.iijournals.com/content/24/2/11},
volume = {24},
year = {1998}
}
@book{Graham1934,
abstract = {292 pages : illustrations ; 22 cm},
author = {Graham, Benjamin},
publisher = {[Second revised edition]. New York : Harper, [1959]},
title = {{The intelligent investor: a book of practical counsel}},
url = {https://search.library.wisc.edu/catalog/999688336402121},
year = {1934}
}
@article{FAMA1992a,
abstract = {This paper studies the cross-sectional properties of return fore-casts derived from Fama-MacBeth regressions. These forecasts mimic how an investor could, in real time, combine many firm characteristics to obtain a composite estimate of a stock's expected return. Empirically, the forecasts vary substantially across stocks and have strong predictive power for actual re-turns. For example, using ten-year rolling estimates of Fama-MacBeth slopes and a cross-sectional model with 15 firm charac-teristics (all based on low-frequency data), the expected-return estimates have a cross-sectional standard deviation of 0.87{\%} monthly and a predictive slope for future monthly returns of 0.74, with a standard error of 0.07.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {FAMA, EUGENE F. and FRENCH, KENNETH R.},
doi = {10.1111/j.1540-6261.1992.tb04398.x},
eprint = {1011.1669},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/FAMA, FRENCH - 1992 - The Cross‐Section of Expected Stock Returns(2).pdf:pdf},
isbn = {00221082},
issn = {15406261},
journal = {The Journal of Finance},
pmid = {2329112},
title = {{The Cross‐Section of Expected Stock Returns}},
year = {1992}
}
@techreport{Li2004,
abstract = {Different strategies for binary analysis are widely used in systems dealing with software maintenance and system security. Binary code is self-contained; though it is easy to execute, it is not easy to read and understand. Binary analysis tools are useful in software maintenance because the binary of software has all the information necessary to recover the source code. It is also incredibly important and sensitive in the domain of security. Malicious binary code can infect other applications, hide in their binary code, contaminate the whole system or travel through Internet and attack other systems. This makes it imperative for security personnel to scan and analyze binary codes with the aid of the binary code analysis tools. On the other hand, crackers can reverse engineer the binary code to assembly code in order to break the secrets embedded in the binary code, such as registration number, password or secret algorithms. This motivates researches to prevent malicious monitoring by binary code analysis tools. Evidently, binary analysis tools play an important double-sided role in security. This paper surveys binary code analysis from the most fundamental perspective views: the binary code formats, several of the most basic analysis tools, such as disassembler, debugger and the instrumentation tools based on them. The previous research on binary analysis are investigated and summarized and a new approach of analysis, disasembler-based binary interpreter, is proposed and discussed.},
author = {Li, Shengying},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li - 2004 - A Survey on Tools for Binary Code Analysis.pdf:pdf},
title = {{A Survey on Tools for Binary Code Analysis}},
year = {2004}
}
@article{Stodden2010,
abstract = {Roundtable participants identified ways of making computational research details readily available, which is a crucial step in addressing the current credibility crisis.},
author = {Stodden, Victoria},
doi = {10.1109/MCSE.2010.113},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2010 - Reproducible Research.pdf:pdf},
isbn = {1521-9615},
issn = {1521-9615},
journal = {Computing in Science {\&} Engineering},
pmid = {20498225},
title = {{Reproducible Research}},
year = {2010}
}
@unpublished{Gentleman2004,
abstract = {For various reasons, it is important, if not essential, to integrate the computations and code used in data analyses, methodological descriptions, simulations, etc. with the documents that describe and rely on them. This integration allows readers to both verify and adapt the statements in the documents. Authors can easily reproduce them in the future, and they can present the document's contents in a different medium, e.g. with interactive controls. This paper describes a software framework for authoring and distributing these integrated, dynamic documents that contain text, code, data, and any auxiliary content needed to recreate the computations. The documents are dynamic in that the contents, including figures, tables, etc., can be recalculated each time a view of the document is generated. Our model treats a dynamic document as a master or ``source'' document from which one can generate different views in the form of traditional, derived documents for different audiences. We introduce the concept of a compendium as both a container for the different elements that make up the document and its computations (i.e. text, code, data, ...), and as a means for distributing, managing and updating the collection. The step from disseminating analyses via a compendium to reproducible research is a small one. By reproducible research, we mean research papers with accompanying software tools that allow the reader to directly reproduce the results and employ the methods that are presented in the research paper. Some of the issues involved in paradigms for the production, distribution and use of such reproducible research are discussed.},
author = {Gentleman, Robert and Lang, Duncan},
booktitle = {Bioconductor Project Working Papers},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gentleman, Lang - 2004 - Statistical Analyses and Reproducible Research.pdf:pdf},
title = {{Statistical Analyses and Reproducible Research}},
year = {2004}
}
@article{Ihaka1998,
abstract = {R began as an experiment in trying to use the methods of Lisp implementors$\backslash$nto build a small testbed which could be used to trial some ideas$\backslash$non how a statistical environment might be built. Early on, the decision$\backslash$nwas made to use an S-like syntax. Once that decision was made, the$\backslash$nmove toward being more and more like S has been irresistible.$\backslash$n$\backslash$n$\backslash$nR has now outgrown its origins and its development is now a collaborative$\backslash$neffort undertaken using the Internet to exchange ideas and distribute$\backslash$nthe results. The focus is now on how the initial experiment can be$\backslash$nturned into a viable piece of free software.$\backslash$n$\backslash$n$\backslash$nThis paper reviews the past and present status of R and takes a brief$\backslash$nlook at where future development might lead.},
address = {Aukland},
author = {Ihaka, Ross},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - R Past and Future History.pdf:pdf},
institution = {University of Aukland},
journal = {Proceedings of the 30th Symposium on the Interface},
pages = {392--396},
title = {{R: Past and Future History}},
url = {http://cran.r-project.org/doc/html/interface98-paper/paper.html},
year = {1998}
}
@inproceedings{Safarti2015,
author = {Safarti, Olivier (Citigroup)},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Safarti - 2015 - Backtesting A Practitioner's Gide to Assessing Strategies and Avoiding Pitfalls.pdf:pdf},
title = {{Backtesting : A Practitioner's Gide to Assessing Strategies and Avoiding Pitfalls}},
year = {2015}
}
@article{Campbell2007,
abstract = {This paper reviews a variety of backtests that examine the adequacy of Value-at- Risk (VaR) measures. These backtesting procedures are reviewed from both a sta- tistical and risk management perspective. The properties of unconditional coverage and independence are dened and their relation to backtesting procedures is discussed. Backtests are then classied by whether they examine the unconditional coverage prop- erty, independence property, or both properties of a VaR measure. Backtests that ex- amine the accuracy of a VaR model at several quantiles, rather than a single quantile, are also outlined and discussed. The statistical power properties of these tests are examined in a simulation experiment. Finally, backtests that are specied in terms of a pre-specied loss function are reviewed and their use in VaR validation is discussed.},
author = {Campbell, Sean},
doi = {10.21314/JOR.2007.146},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Campbell - 2007 - A review of backtesting and backtesting procedures.pdf:pdf},
isbn = {14651211},
issn = {14651211},
journal = {The Journal of Risk},
number = {2},
pages = {1--17},
pmid = {23622121},
title = {{A review of backtesting and backtesting procedures}},
url = {http://www.risk.net/journal-of-risk/technical-paper/2160965/a-review-backtesting-backtesting-procedures},
volume = {9},
year = {2007}
}
@techreport{Broman,
abstract = {Spreadsheets are widely used software tools for data entry, storage, analysis, and visualization. Focusing on the data entry and storage aspects, this article offers practical recommendations for organizing spreadsheet data to reduce errors and ease later analyses. The basic principles are: be consistent, write dates like YYYY-MM-DD, do not leave any cells empty, put just one thing in a cell, organize the data as a single rectangle (with subjects as rows and variables as columns, and with a single header row), create a data dictionary, do not include calculations in the raw data files, do not use font color or highlighting as data, choose good names for things, make backups, use data validation to avoid data entry errors, and save the data in plain text files.},
author = {Broman, Karl W and Woo, Kara H},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Broman, Woo - Unknown - Data Organization in Spreadsheets.pdf:pdf},
keywords = {Data management,Data organization,Microsoft Excel,Spreadsheets},
title = {{Data Organization in Spreadsheets}},
url = {https://doi.org/./..}
}
@misc{Wickham2016,
annote = {R package version 0.3.1},
author = {Wickham, Hadley},
title = {{feather: R Bindings to the Feather 'API'}},
url = {https://cran.r-project.org/package=feather},
year = {2016}
}
@misc{Bache2014,
annote = {R package version 1.5},
author = {Bache, Stefan Milton and Wickham, Hadley},
title = {{magrittr: A Forward-Pipe Operator for R}},
url = {https://cran.r-project.org/package=magrittr},
year = {2014}
}
@misc{RStudioTeam2015,
address = {Boston, MA},
author = {{RStudio Team}},
title = {{RStudio: Integrated Development Environment for R}},
url = {http://www.rstudio.com/},
year = {2015}
}
@misc{RCoreTeam2018,
address = {Vienna, Austria},
author = {{R Core Team}},
title = {{R: A Language and Environment for Statistical Computing}},
url = {https://www.r-project.org/},
year = {2018}
}
@misc{Katzke2017,
address = {Stellenbosch, South Africa},
annote = {version 0.1.2},
author = {Katzke, N F},
title = {{Texevier: Package to create Elsevier templates for Rmarkdown}},
year = {2017}
}
@misc{Wickham2017,
annote = {R package version 1.2.1},
author = {Wickham, Hadley},
title = {{tidyverse: Easily Install and Load the 'Tidyverse'}},
url = {https://cran.r-project.org/package=tidyverse},
year = {2017}
}
@article{Marwick2018,
abstract = {Computers are a central tool in the research process, enabling complex and large scale data analysis. As computer-based research has increased in complexity, so have the challenges of ensuring that this research is reproducible. To address this challenge, we review the concept of the research compendium as a solution for providing a standard and easily recognisable way for organising the digital materials of a research project to enable other researchers to inspect, reproduce, and extend the research. We investigate how the structure and tooling of software packages of the R programming language are being used to produce research compendia in a variety of disciplines. We also describe how software engineering tools and services are being used by researchers to streamline working with research compendia. Using real-world examples, we show how researchers can improve the reproducibility of their work using research compendia based on R packages and related tools.},
author = {Marwick, Ben and Boettiger, Carl and Mullen, Lincoln},
doi = {10.1080/00031305.2017.1375986},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Marwick, Boettiger, Mullen - 2018 - Packaging Data Analytical Work Reproducibly Using R (and Friends).pdf:pdf},
issn = {15372731},
journal = {American Statistician},
keywords = {Computational science,Data science,Open source software,Reproducible research},
title = {{Packaging Data Analytical Work Reproducibly Using R (and Friends)}},
year = {2018}
}
@techreport{Stodden,
abstract = {There is a gap in the current licensing and copyright structure for the growing number of scientists releasing their research publicly, particularly on the Internet. Scientific research produces more scholarship than the final paper: for example, the code, data structures, experimental design and parameters, documentation, and figures, are all important both for communication of the scholarship and replication of the results. US copyright law is a barrier to the sharing of scientific scholarship since it establishes exclusive rights for creators over their work, thereby limiting the ability of others to copy, use, build upon, or alter the research. This is precisely opposite to prevailing scientific norms, which provide both that results be replicated before accepted as knowledge, and that scientific understanding be built upon previous discoveries for which authorship recognition is given. In accordance with these norms and to encourage the release of all scientific scholarship, I propose the Reproducible Research Standard (RRS) both to ensure attribution and facilitate the sharing of scientific works. Using the RRS on all components of scientific scholarship will encourage reproducible scientific investigation, facilitate greater collaboration, and promote engagement of the larger community in scientific learning and discovery.},
author = {Stodden, Victoria and Bitton, Miriam and Donoho, David and Hillis, Danny and Lessig, Larry and Palfrey, John and Wilbanks, John},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stodden et al. - Unknown - ENABLING REPRODUCIBLE RESEARCH OPEN LICENSING FOR SCIENTIFIC INNOVATION am very grateful for invaluable discu.pdf:pdf},
title = {{ENABLING REPRODUCIBLE RESEARCH: OPEN LICENSING FOR SCIENTIFIC INNOVATION am very grateful for invaluable discussion with Licensing in the Sciences}},
url = {http://ssrn.com/abstract=1362040.Ofcourse,anyerrorsareminealone.Electroniccopyavailableat:http://ssrn.com/abstract=1362040}
}
@article{Peng2009,
abstract = {10.1093/biostatistics/kxp014},
archivePrefix = {arXiv},
arxivId = {10.1093/biostatistics/kxp014},
author = {Peng, Roger D.},
doi = {10.1093/biostatistics/kxp014},
eprint = {biostatistics/kxp014},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Peng - 2009 - Reproducible research and Biostatistics.pdf:pdf},
isbn = {1465-4644},
issn = {14654644},
journal = {Biostatistics},
pmid = {19535325},
primaryClass = {10.1093},
title = {{Reproducible research and Biostatistics}},
year = {2009}
}
@article{Koenker2009,
abstract = {Recent software developments are reviewed from the vantage point of reproducible econometric research. We argue that the emergence of new tools, particularly in the open-source community, have greatly eased the burden of documenting and archiving both empirical and simulation work in econometrics. Some of these tools are highlighted in the discussion of two small replication exercises.},
author = {Koenker, Roger and Zeileis, Achim},
doi = {10.1002/jae.1083},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koenker, Zeileis - 2009 - On reproducible econometric research.pdf:pdf},
isbn = {08837252},
issn = {08837252},
journal = {Journal of Applied Econometrics},
pmid = {5011887359496252936},
title = {{On reproducible econometric research}},
year = {2009}
}
@misc{Baum2002,
author = {Baum, Christopher F. and Sirin, Selcuk},
title = {{Why should you avoid using point-and-click method in statistical software packages?}},
url = {http://fmwww.bc.edu/GStat/docs/pointclick.html},
urldate = {2018-11-09},
year = {2002}
}
@article{Wickham2014,
abstract = {In this paper we present the R package gRain for propagation in graphical indepen- dence networks (for which Bayesian networks is a special instance). The paper includes a description of the theory behind the computations. The main part of the paper is an illustration of how to use the package. The paper also illustrates how to turn a graphical model and data into an independence network.},
archivePrefix = {arXiv},
arxivId = {arXiv:1501.0228},
author = {Wickham, Hadley},
doi = {10.18637/jss.v059.i10},
eprint = {arXiv:1501.0228},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wickham - 2014 - Tidy Data.pdf:pdf},
isbn = {9780387781662},
issn = {1548-7660},
journal = {Journal of Statistical Software},
pmid = {18291371},
title = {{Tidy Data}},
year = {2014}
}
@article{Baumer2014,
abstract = {Nolan and Temple Lang argue that "the ability to express statistical computations is an essential skill." A key related capacity is the ability to conduct and present data analysis in a way that another person can understand and replicate. The copy-and-paste workflow that is an artifact of antiquated user-interface design makes reproducibility of statistical analysis more difficult, especially as data become increasingly complex and statistical methods become increasingly sophisticated. R Markdown is a new technology that makes creating fully-reproducible statistical analysis simple and painless. It provides a solution suitable not only for cutting edge research, but also for use in an introductory statistics course. We present evidence that R Markdown can be used effectively in introductory statistics courses, and discuss its role in the rapidly-changing world of statistical computation.},
archivePrefix = {arXiv},
arxivId = {1402.1894},
author = {Baumer, Ben and Cetinkaya-Rundel, Mine and Bray, Andrew and Loi, Linda and Horton, Nicholas J.},
doi = {10.5811/westjem.2011.5.6700},
eprint = {1402.1894},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baumer et al. - 2014 - R Markdown Integrating A Reproducible Analysis Tool into Introductory Statistics.pdf:pdf},
isbn = {9780980200454},
issn = {1933-4214},
pmid = {24696751},
title = {{R Markdown: Integrating A Reproducible Analysis Tool into Introductory Statistics}},
year = {2014}
}
@book{Salus1994,
abstract = {UNIX is a software system that is simple, elegant, portable, and powerful. It grew in popularity without the benefit of a large marketing organization. Programmers kept using it; big companies kept fighting it. After a decade, it was clear that the users had won. A Quarter Century of UNIX is the first book to explain this incredible success, using the words of its creators, developers and users to illustrate how the sociology of a technical group can overwhelm the intent of multi-billion-dollar corporations. In preparing to write this book, Peter Salus interviewed over 100 of these key figures and gathered relevant information from Australia to Austria. This is the book that turns UNIX folklore into UNIX history. Features: provides the first documented history of the development of the UNIX operating system, includes interviews with over 100 key figures in the UNIX community, contains classic photos and illustrations, and explains why UNIX succeeded.},
author = {Salus, Peter H.},
isbn = {0201547775},
pages = {256},
publisher = {Addison-Wesley Pub. Co},
title = {{A quarter century of UNIX}},
url = {https://dl.acm.org/citation.cfm?id=191771{\&}preflayout=tabs},
year = {1994}
}
@misc{Munafo2017,
abstract = {Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
author = {Munaf{\`{o}}, Marcus R and Nosek, Brian A and Bishop, Dorothy V.M. and Button, Katherine S and Chambers, Christopher D and {Percie Du Sert}, Nathalie and Simonsohn, Uri and Wagenmakers, Eric Jan and Ware, Jennifer J and Ioannidis, John P.A.},
booktitle = {Nature Human Behaviour},
doi = {10.1038/s41562-016-0021},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Munaf{\`{o}} et al. - 2017 - A manifesto for reproducible science.pdf:pdf},
isbn = {4156201600},
issn = {23973374},
number = {1},
title = {{A manifesto for reproducible science}},
url = {https://www.nature.com/articles/s41562-016-0021.pdf},
volume = {1},
year = {2017}
}
@article{LopezdePrado2013,
abstract = {Most firms and portfolio managers rely on backtests (or historical simulations of performance) to select investment strategies and allocate them capital. Standard statistical techniques designed to prevent regression over-fitting, such as hold-out, tend to be unreliable and inaccurate in the context of investment backtests. We propose a framework that estimates the probability of backtest over-fitting (PBO) specifically in the context of investment simulations, through a numerical method that we call combinatorially symmetric cross-validation (CSCV). We show that CSCV produces accurate estimates of the probability that a particular backtest is over-fit.},
author = {{Lopez de Prado}, Marcos},
doi = {10.2139/ssrn.2308682},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lopez de Prado - 2013 - The Probability of Back-Test Over-Fitting.pdf:pdf;:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lopez de Prado - 2013 - The Probability of Back-Test Over-Fitting(2).pdf:pdf},
isbn = {047125391X},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
title = {{The Probability of Back-Test Over-Fitting}},
url = {http://www.ssrn.com/abstract=2308682},
year = {2013}
}
@article{Franke2016,
abstract = {The need for new methods to deal with big data is a common theme in most scientific fields, although its definition tends to vary with the context. Statistical ideas are an essential part of this, and as a partial response, a thematic program on statistical inference, learning and models in big data was held in 2015 in Canada, under the general direction of the Canadian Statistical Sciences Institute, with major funding from, and most activities located at, the Fields Institute for Research in Mathematical Sciences. This paper gives an overview of the topics covered, describing challenges and strategies that seem common to many different areas of application and including some examples of applications to make these challenges and strategies more concrete.},
archivePrefix = {arXiv},
arxivId = {1509.02900},
author = {Franke, Beate and Plante, Jean-Fran{\c{c}}ois and Roscher, Ribana and Lee, En-shiun Annie and Smyth, Cathal and Hatefi, Armin and Chen, Fuqi and Gil, Einat and Schwing, Alexander and Selvitella, Alessandro and Hoffman, Michael M. and Grosse, Roger and Hendricks, Dieter and Reid, Nancy},
doi = {10.1111/insr.12176},
eprint = {1509.02900},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Franke et al. - 2016 - Statistical Inference, Learning and Models in Big Data.pdf:pdf},
issn = {03067734},
journal = {International Statistical Review},
keywords = {aggregation,computational complexity,dimension reduction,high-dimensional data,networks,streaming data},
number = {3},
pages = {371--389},
title = {{Statistical Inference, Learning and Models in Big Data}},
url = {http://doi.wiley.com/10.1111/insr.12176},
volume = {84},
year = {2016}
}
@article{Langkvist2014,
abstract = {This paper gives a review of the recent developments in deep learning and unsupervised feature learning for time-series problems. While these techniques have shown promise for modeling static data, such as computer vision, applying them to time-series data is gaining increasing attention. This paper overviews the particular challenges present in time-series data and provides a review of the works that have either applied time-series data to unsupervised feature learning algorithms or alternatively have contributed to modifications of feature learning algorithms to take into account the challenges present in time-series data. {\textcopyright} 2014 Elsevier Ltd.},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {L{\"{a}}ngkvist, Martin and Karlsson, Lars and Loutfi, Amy},
doi = {10.1016/j.patrec.2014.01.008},
eprint = {1602.07261},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/L{\"{a}}ngkvist, Karlsson, Loutfi - 2014 - A review of unsupervised feature learning and deep learning for time-series modeling.pdf:pdf},
isbn = {0167-8655},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Deep learning,Time-series,Unsupervised feature learning},
number = {1},
pages = {11--24},
pmid = {23064159},
title = {{A review of unsupervised feature learning and deep learning for time-series modeling}},
volume = {42},
year = {2014}
}
@article{Ross2002,
abstract = {No abstract available.},
author = {Ross, Steven A.},
doi = {10.1111/1468-036X.00181},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ross - 2002 - Neoclassical finance, alternative finance and the closed end fund puzzle.pdf:pdf},
isbn = {1468-036X},
issn = {1468036X},
journal = {European Financial Management},
title = {{Neoclassical finance, alternative finance and the closed end fund puzzle}},
year = {2002}
}
@book{Ross2005,
abstract = {EBIB.E72.A0290 check priv{\'{e}}},
author = {Ross, Stephen A},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ross - 2005 - Neoclassical Finance.pdf:pdf},
isbn = {0691121389},
pages = {120 pp.},
pmid = {13671762},
title = {{Neoclassical Finance}},
year = {2005}
}
@article{Jarrow2012,
abstract = {We improve upon the power of the statistical arbitrage test in Hogan, Jarrow, Teo, and Warachka (2004). Our methodology also allows for the evaluation of return anomalies under weaker assumptions. We then compare strategies based on their convergence rates to arbitrage and identify strategies whose probability of a loss declines to zero most rapidly. These strategies are preferred by investors with finite horizons or limited capital. After controlling for market frictions and examining convergence rates to arbitrage, we find that momentum and value strategies offer the most desirable trading opportunities. {\textcopyright} 2011 Elsevier B.V.},
author = {Jarrow, Robert and Teo, Melvyn and Tse, Yiu Kuen and Warachka, Mitch},
doi = {10.1016/j.finmar.2011.08.003},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jarrow et al. - 2012 - An improved test for statistical arbitrage.pdf:pdf},
issn = {13864181},
journal = {Journal of Financial Markets},
keywords = {Bootstrap,Momentum strategy,Statistical arbitrage,Value strategy},
number = {1},
pages = {47--80},
publisher = {Elsevier},
title = {{An improved test for statistical arbitrage}},
url = {http://dx.doi.org/10.1016/j.finmar.2011.08.003},
volume = {15},
year = {2012}
}
@article{Farmer2009,
abstract = {The use of equilibrium models in economics springs from the desire for parsimonious models of economic phenomena that take human reasoning into account. This approach has been the cornerstone of modern economic theory. We explain why this is so, extolling the virtues of equilibrium theory; then we present a critique and describe why this approach is inherently limited, and why economics needs to move in new directions if it is to continue to make progress. We stress that this shouldn't be a question of dogma, but should be resolved empirically. There are situations where equilibrium models provide useful predictions and there are situations where they can never provide useful predictions. There are also many situations where the jury is still out, i.e., where so far they fail to provide a good description of the world, but where proper extensions might change this. Our goal is to convince the skeptics that equilibrium models can be useful, but also to make traditional economists more aware of the limitations of equilibrium models. We sketch some alternative approaches and discuss why they should play an important role in future research in economics.},
archivePrefix = {arXiv},
arxivId = {0803.2996},
author = {Farmer, J. Doyne and Geanakoplos, John},
doi = {10.1002/cplx.20261},
eprint = {0803.2996},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Farmer, Geanakoplos - 2009 - The virtues and vices of equilibrium and the future of financial economics.pdf:pdf},
isbn = {1076-2787},
issn = {10762787},
journal = {Complexity},
keywords = {Agent-based modeling,Arbitrage,Bounded rationality,Disequilibrium,Efficiency,Equilibrium,Market ecology,Power laws,Rational expectations,Zero intelligence},
number = {3},
pages = {11--38},
pmid = {20332},
title = {{The virtues and vices of equilibrium and the future of financial economics}},
volume = {14},
year = {2009}
}
@misc{BLACK1986,
abstract = {The effects of noise on the world, and on our views of the world, are profound, Noise in the sense of a large number of small events is often a causal factor much more powerful than a small number of large events can be. Noise makes trading in financial markets possible, and thus allows us to observe prices for financial assets. Noise causes markets to be somewhat inefficient, but often prevents us from taking advantage of ineffiencies. Noise in the form of uncertainty about future tases and technology by sector causes business cycles, and makes them highly resistant to improvement through government intervention. Noise in in the form of expectations that need not follow rational rules causes inflation to be what it is, at least in the absense of a gold standard or fixed exchange rates. Noise in the form of uncertainty about what relative prices would be with other exchange rates makes us think incorrectly that changes in exchange rates or inflation rates cause changes in trade or investment flows or economic activity. Most generally, noise makes it very difficult to test either practical or academic theories about the way that financial or economic markets work. We are forced to act largely in the dark.},
author = {BLACK, FISCHER},
booktitle = {The Journal of Finance},
doi = {10.1111/j.1540-6261.1986.tb04513.x},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/BLACK - 1986 - Noise.pdf:pdf},
isbn = {1540-6261},
issn = {15406261},
number = {3},
pages = {529--543},
title = {{Noise}},
volume = {41},
year = {1986}
}
@misc{Daniel1999,
author = {Daniel, Kent and Wei, K.C. and Titman, Sheridan},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Daniel, Wei, Titman - 1999 - Explaining the Cross-Section of Stock Returns in Japan Factors or Characteristics.pdf:pdf},
month = {jul},
title = {{Explaining the Cross-Section of Stock Returns in Japan: Factors or Characteristics?}},
url = {https://papers.ssrn.com/sol3/papers.cfm?abstract{\_}id=202433},
year = {1999}
}
@article{Daniel2001,
abstract = {Japanese stock returns are even more closely related to their book-to-market ratios than are their U.S. counterparts, and thus provide a good setting for testing whether the return premia associated with these characteristics arise because the charac- teristics are proxies for covariance with priced factors. Our tests, which replicate the Daniel and Titman {\~{}}1997! tests on a Japanese sample, reject the Fama and French {\~{}}1993! three-factor model, but fail to reject the characteristic model.},
author = {Daniel, Kent and Titman, Sheridan and Wei, K. C.John},
doi = {10.1111/0022-1082.00344},
isbn = {00221082},
issn = {00221082},
journal = {Journal of Finance},
number = {2},
pages = {743--766},
pmid = {222581},
publisher = {WileyAmerican Finance Association},
title = {{Explaining the cross-section of stock returns in Japan: Factors or characteristics?}},
url = {https://www.jstor.org/stable/222581},
volume = {56},
year = {2001}
}
@article{Sylvain2013,
abstract = {I reproduce the results of Fama and MacBeth (1973) and extend this paper in several ways. First, I use twenty-five test portfolios constructed using a double-sort on betas and standard deviations of the residuals of the underlying securities (while still maintaining the same timing assumptions and method of Fama and MacBeth; 1973). I find that the coefficient on the average standard deviation of the residuals becomes significant in the cross-sectional regressions, even for the 1929-1968 time period. This goes against some of the results in Fama and MacBeth (1973) and hints that these results may not have been robust. In particular, I alternate between using an equal-weighted and value-weighted NYSE portfolio as proxy for the market portfolio. I also alternate the construction of the test portfolio estimates between equal-weighting and value-weighting of the underlying securities estimates. I find that the Fama-MacBeth (1973) results are unaffected by these variations if the test portfolios are constructed using a single sort on betas as done in Fama-MacBeth (1973). But with test portfolios built with a dual-sort much of the Fama-MacBeth (1973) results are undone. ¦ I am grateful to Aaron Foss for providing me with helpful comments, suggestions, and feedback.},
author = {Sylvain, Serginio},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sylvain - 2013 - Fama-MacBeth 1973 Replication and Extension.pdf:pdf},
title = {{Fama-MacBeth 1973: Replication and Extension}},
url = {http://home.uchicago.edu/{~}serginio/research/FamaPaper{\_}fm73replication{\_}extension.pdf},
year = {2013}
}
@article{Fama1973,
abstract = {This paper tests the relationship between average return and risk for New York Stock Exchange common stocks. The theoretical basis of the tests is the "two-parameter" portfolio model and models of market equilibrium derived from the two-parameter portfolio model. We cannot reject the hypothesis of these models that the pricing of common stocks reflects the attempts of risk-averse investors to hold portfolios that are "efficient" in terms of expected value and dispersion of return. Moreover, the observed "fair game" properties of the coefficients and residuals of the risk-return regressions are consistent with an "efficient capital market"--that is, a market where prices of securities},
author = {Fama, Eugene F. and MacBeth, James D.},
doi = {10.2307/1831028},
journal = {Journal of Political Economy},
pages = {607--636},
publisher = {The University of Chicago Press},
title = {{Risk, Return, and Equilibrium: Empirical Tests}},
url = {https://www.jstor.org/stable/1831028},
volume = {81},
year = {1973}
}
@article{Daniel1997,
abstract = {Firm sizes and book-to-market ratios are both highly correlated with the average returns of common stocks. Fama and French (1993) argue that the association between these characteristics and returns arise because the characteristics are proxies for nondiversifiable factor risk. In contrast, the evidence in this article indicates that the return premia on small capitalization and high book-to-market stocks does not arise because of the comovements of these stocks with pervasive factors. It is the characteristics rather than the covariance structure of returns that appear to explain the cross-sectional variation in stock returns.},
author = {Daniel, Kent and Titman, Sheridan},
doi = {10.2307/2329554},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Daniel, Titman - 1997 - Evidence on the Characteristics of Cross Sectional Variation in Stock Returns.pdf:pdf;:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Daniel, Titman - 1997 - Evidence on the Characteristics of Cross Sectional Variation in Stock Returns(2).pdf:pdf},
isbn = {00221082},
issn = {00221082},
journal = {The Journal of Finance},
number = {1},
pages = {1},
pmid = {2329554},
title = {{Evidence on the Characteristics of Cross Sectional Variation in Stock Returns}},
url = {http://www.jstor.org/stable/2329554?origin=crossref},
volume = {52},
year = {1997}
}
@misc{Fama1992,
abstract = {Two easily measured variables, size and book-to-market equity, combine to capture the cross-sectional variation in average stock returns associated with market beta, size, leverage, book-to-market equity, and earings-price ratios. Moreover, when the tests allow for variation in beta that is unrelated to size, the relation between market beta and average return is flat, even when beta is the only explanatory variable.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Fama, E. and French, K.},
booktitle = {The Journal of Finance},
doi = {10.2307/2329112},
eprint = {1011.1669},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fama, French - 1992 - The Cross-Section of Expected Stock Returns.pdf:pdf},
isbn = {00221082},
issn = {1540-6261},
number = {2},
pages = {427--465},
pmid = {2329112},
title = {{The Cross-Section of Expected Stock Returns}},
volume = {47},
year = {1992}
}
@article{Fielding2000,
author = {Fielding, Roy Thomas},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fielding - 2000 - UNIVERSITY OF CALIFORNIA, IRVINE Architectural Styles and the Design of Network-based Software Architectures.pdf:pdf},
title = {{UNIVERSITY OF CALIFORNIA, IRVINE Architectural Styles and the Design of Network-based Software Architectures}},
year = {2000}
}
@misc{Bloomberg2014,
author = {Bloomberg},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2014 - Bloomberg API.pdf:pdf},
issn = {0006-3495},
title = {{Bloomberg API Version 3.x Developer's Guide}},
year = {2014}
}
@article{Kraska2017,
abstract = {Indexes are models: a B-Tree-Index can be seen as a model to map a key to the position of a record within a sorted array, a Hash-Index as a model to map a key to a position of a record within an unsorted array, and a BitMap-Index as a model to indicate if a data record exists or not. In this exploratory research paper, we start from this premise and posit that all existing index structures can be replaced with other types of models, including deep-learning models, which we term learned indexes. The key idea is that a model can learn the sort order or structure of lookup keys and use this signal to effectively predict the position or existence of records. We theoretically analyze under which conditions learned indexes outperform traditional index structures and describe the main challenges in designing learned index structures. Our initial results show, that by using neural nets we are able to outperform cache-optimized B-Trees by up to 70{\%} in speed while saving an order-of-magnitude in memory over several real-world data sets. More importantly though, we believe that the idea of replacing core components of a data management system through learned models has far reaching implications for future systems designs and that this work just provides a glimpse of what might be possible.},
archivePrefix = {arXiv},
arxivId = {1712.01208},
author = {Kraska, Tim and Beutel, Alex and Chi, Ed H. and Dean, Jeffrey and Polyzotis, Neoklis},
eprint = {1712.01208},
title = {{The Case for Learned Index Structures}},
url = {http://arxiv.org/abs/1712.01208},
year = {2017}
}
@article{Meerkamp2016,
abstract = {We present an architecture for information extraction from text that augments an existing parser with a character-level neural network. The network is trained using a measure of consistency of extracted data with existing databases as a form of noisy supervision. Our architecture combines the ability of constraint-based information extraction systems to easily incorporate domain knowledge and constraints with the ability of deep neural networks to leverage large amounts of data to learn complex features. Boosting the existing parser's precision, the system led to large improvements over a mature and highly tuned constraint-based production information extraction system used at Bloomberg for financial language text.},
archivePrefix = {arXiv},
arxivId = {1612.04118},
author = {Meerkamp, Philipp and Zhou, Zhengyi},
eprint = {1612.04118},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Meerkamp, Zhou - 2016 - Information Extraction with Character-level Neural Networks and Free Noisy Supervision.pdf:pdf},
title = {{Information Extraction with Character-level Neural Networks and Free Noisy Supervision}},
url = {http://arxiv.org/abs/1612.04118},
year = {2016}
}
@article{Valkenburgh2016,
abstract = {This report offers a non-technical but thorough explanation of " blockchain technology " with a focus on the key variables within consensus mechanism design that differentiate so-called permissioned or closed and permissionless or open blockchains. We describe decentralized computing generally and draw parallels between open blockchain networks, e.g. Bitcoin, Ethereum, and Zcash, and the early Internet. For certain use cases, we explain why open networks may be more worthy of user trust and more capable of ensuring user privacy and security. Our highlighted use cases are electronic cash, digital identity, and the Internet of Things. Electronic cash promises efficient microtransactions, and enhanced financial inclusion; robust digital identity may solve many of our online security woes and streamline commerce and interaction online; and blockchain-driven Internet of Things systems may spur greater security, competition, and an end to walled gardens of non-interoperability for connected devices. We argue that the full benefits of these potential use cases can only be realized by utilizing open blockchain networks.},
author = {Valkenburgh, Peter Van},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Valkenburgh - 2016 - Open Matters Why Permissionless Blockchains are Essential to the Future of the Internet.pdf:pdf},
title = {{Open Matters Why Permissionless Blockchains are Essential to the Future of the Internet}},
year = {2016}
}
@article{Cohen1996,
abstract = {We refine the description of the characteristics that best explain cross-sectional and common variation in expected stock returns. By breaking BE/ME into its inter-and intra-industry components, we find that the book-to-market effect is primarily intra-industry, in stark contrast to results for the momentum effect. We create factor-mimicking portfolios that imply an extremely high ex post price of risk, and appear to be more efficient than the simple book-to-market factor-mimicking portfolio of Fama and French (1993). We then use these intra-industry portfolios to test the characteristic-based asset-pricing model of Daniel and Titman (1997). Our results are inconsistent with that behavioral model.},
author = {Cohen, Randolph B and Polk, Christopher K},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohen, Polk - 1996 - An investigation of the impact of industry factors in asset-pricing tests.pdf:pdf},
journal = {Mimeo},
keywords = {CAPM-empirisch,diversificatie-sector},
number = {June 1994},
pages = {30+},
title = {{An investigation of the impact of industry factors in asset-pricing tests}},
volume = {02163},
year = {1996}
}
@article{Dedeo2017,
author = {Dedeo, Simon},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dedeo - 2017 - Bayesian Reasoning for Intelligent People.pdf:pdf},
title = {{Bayesian Reasoning for Intelligent People}},
year = {2017}
}
@misc{Bessembinder2017,
author = {Bessembinder, Hendrik},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bessembinder - 2017 - Do Stocks Outperform Treasury Bills.pdf:pdf},
keywords = {Long horizon returns,return skewness,stock market wealth creation},
month = {nov},
title = {{Do Stocks Outperform Treasury Bills?}},
url = {https://papers.ssrn.com/sol3/papers.cfm?abstract{\_}id=2900447},
year = {2017}
}
@article{Daniel2017,
abstract = {Preliminary: Please do not quote without permission -Abstract -In the finance literature, a common practice is to create factor-portfolios by sorting on characteristics associated with average returns. The goal of this exercise is to create a set of portfolios that explain the cross-section of average returns. This result obtains if and only if the set of factor-portfolios span the mean-variance efficient portfolio. We argue that this is unlikely to be the case, as factor portfolios constructed in this way fail to incorporate information about the covariance structure of returns. By using a high statistical power methodology to forecast future covariances, we are able to construct a set of portfolios which capture the characteristic premia, but hedge out much of factor risk. We apply our methodology to the Fama and French (2015) five-factors, and construct a portfolio orthogonal to their factor with annualized Sharpe-ratio of 0.84.},
author = {Daniel, Kent and Mota, Lira and Rottke, Simon and Santos, Tano and Lochstoer, Lars and Jagannathan, Ravi and Tetlock, Paul and Weller, Brian},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Daniel et al. - 2017 - The Cross-Section of Risk and Return.pdf:pdf},
title = {{The Cross-Section of Risk and Return}},
url = {http://www.kentdaniel.net/papers/unpublished/dmrs.pdf},
year = {2017}
}
@article{DANIEL1997,
abstract = {This article develops and applies new measures of portfolio performance which use benchmarks based on the characteristics of stocks held by the portfolios that are evaluated. Specifically, the benchmarks are constructed from the returns of 125 passive portfolios that are matched with stocks held in the evaluated portfolio on the basis of the market capitalization, book-to-market, and prior-year return characteristics of those stocks. Based on these benchmarks, “Characteristic Timing” and “Characteristic Selectivity” measures are developed that detect, respectively, whether portfolio managers successfully time their portfolio weightings on these characteristics and whether managers can select stocks that outperform the average stock having the same characteristics. We apply these measures to a new database of mutual fund holdings covering over 2500 equity funds from 1975 to 1994. Our results show that mutual funds, particularly aggressive-growth funds, exhibit some selectivity ability, but that funds exhibit no characteristic timing ability.},
author = {DANIEL, KENT and GRINBLATT, MARK and TITMAN, SHERIDAN and WERMERS, RUSS},
doi = {10.1111/j.1540-6261.1997.tb02724.x},
isbn = {00221082},
issn = {00221082},
journal = {The Journal of Finance},
number = {3},
pages = {1035--1058},
title = {{Measuring Mutual Fund Performance with Characteristic-Based Benchmarks}},
url = {http://www.kentdaniel.net/papers/published/jf97{\_}2.pdf http://doi.wiley.com/10.1111/j.1540-6261.1997.tb02724.x},
volume = {52},
year = {1997}
}
@article{Heaton2017,
abstract = {We develop a simple stock selection model to explain why active equity managers tend to underperform a benchmark index. We motivate our model with the empirical observation that the best performing stocks in a broad market index often perform much better than the other stocks in the index. Randomly selecting a subset of securi-ties from the index may dramatically increase the chance of underperforming the index. The relative likelihood of underperformance by investors choosing active management likely is much more important than the loss to those same investors from the higher fees for active management relative to passive index investing. Thus, active management may be even more challenging than previously believed, and the stakes for finding the best active managers may be larger than previously assumed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1510.03550v2},
author = {Heaton, J B and {Polson J H Witte}, N G and {Beck Herman Palenchar}, Bartlit},
eprint = {arXiv:1510.03550v2},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heaton, Polson J H Witte, Beck Herman Palenchar - 2017 - Why Indexing Works.pdf:pdf},
keywords = {Active Management,Indexing,Passive Management},
title = {{Why Indexing Works}},
url = {https://arxiv.org/pdf/1510.03550.pdf},
year = {2017}
}
@article{Fama2012,
abstract = {a b s t r a c t In the four regions (North America, Europe, Japan, and Asia Pacific) we examine, there are value premiums in average stock returns that, except for Japan, decrease with size. Except for Japan, there is return momentum everywhere, and spreads in average momentum returns also decrease from smaller to bigger stocks. We test whether empirical asset pricing models capture the value and momentum patterns in interna-tional average returns and whether asset pricing seems to be integrated across the four regions. Integrated pricing across regions does not get strong support in our tests. For three regions (North America, Europe, and Japan), local models that use local explanatory returns provide passable descriptions of local average returns for portfolios formed on size and value versus growth. Even local models are less successful in tests on portfolios formed on size and momentum.},
author = {Fama, Eugene F and French, Kenneth R},
doi = {10.1016/j.jfineco.2012.05.011},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fama, French - 2012 - Size, value, and momentum in international stock returns.pdf:pdf},
keywords = {Four-factor model,Momentum,Three-factor model,Value premium},
title = {{Size, value, and momentum in international stock returns}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.409.2911{\&}rep=rep1{\&}type=pdf},
year = {2012}
}
@article{FAMA1992,
abstract = {ABSTRACT Two easily measured variables, size and book-to-market equity, combine to capture the cross-sectional variation in average stock returns associated with market {\$}\beta{\$}, size, leverage, book-to-market equity, and earnings-price ratios. Moreover, when the tests allow ... {\$}\backslash{\$}n},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {FAMA, EUGENE F. and FRENCH, KENNETH R.},
doi = {10.1111/j.1540-6261.1992.tb04398.x},
eprint = {1011.1669},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/FAMA, FRENCH - 1992 - The Cross‐Section of Expected Stock Returns(2).pdf:pdf},
isbn = {1540-6261},
issn = {15406261},
journal = {The Journal of Finance},
month = {jun},
number = {2},
pages = {427--465},
pmid = {2329112},
publisher = {Blackwell Publishing Ltd},
title = {{The Cross‐Section of Expected Stock Returns}},
url = {http://doi.wiley.com/10.1111/j.1540-6261.1992.tb04398.x},
volume = {47},
year = {1992}
}
@article{Lehmann1988,
abstract = {This paper uses maximum-likelihood factor analysis of large cross-sections to examine the validity of the arbitrage pricing theory (APT). We are unable to explain the expected returns on firm size portfolios, although we do explain the expected returns on portfolios formed on the basis of dividend yield and own variance, where risk adjustment using the usual CAPM market proxies fails. We also compare alternate versions of the APT and sharply reject the hypothesis that basis portfolios formed to mimic the factors span the mean-variance frontier of the individual assets.},
author = {Lehmann, Bruce N. and Modest, David M.},
doi = {10.1016/0304-405X(88)90061-X},
issn = {0304-405X},
journal = {Journal of Financial Economics},
month = {sep},
number = {2},
pages = {213--254},
publisher = {North-Holland},
title = {{The empirical foundations of the arbitrage pricing theory}},
url = {https://www.sciencedirect.com/science/article/pii/0304405X8890061X},
volume = {21},
year = {1988}
}
@article{Lehmann1985,
abstract = {The Arbitrage Pricing Theory (APT) of Ross (1976) presumes that a factor model describes security returns. In this paper, we provide a comprehensive examination of the merits of various strategies for constructing basis portfolios that are, in principle, highly correlated with the common factors affecting security returns. Three main conclusions emerge from our study. First, increasing the number of securities included in the analysis dramatically improves basis portfolio performance. Our results indicate that factor models involving 750 securities provide markedly superior performance to those involving 30 or 250 securities. Second, comparatively efficient estimation procedures such as maximum likelihood and restricted maximum likelihood factor analysis(which imposes the APT mean restriction) significantly outperform the less efficient instrumental variables and principal components procedures that have been proposed in the literature. Third, a variant of the usual Fame-MacBeth portfolio formation procedure, which we call the minimum idiosyncratic risk portfolio formation procedure, outperformed the Fama-MacBeth procedure and proved equal toor better than more expensive quadratic programming procedures.},
address = {Cambridge, MA},
author = {Lehmann, Bruce N. and Modest, David M.},
doi = {10.3386/w1726},
institution = {National Bureau of Economic Research},
journal = {Business},
month = {oct},
title = {{The empirical Foundations of the Arbitrage The Arbitrage Pricing Theory II: the optimal construction of basis portfolios}},
url = {http://www.nber.org/papers/w1726.pdf},
year = {1985}
}
@misc{Fama2009,
author = {Fama, Eugene and French, Kenneth},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fama, French - 2009 - Luck Versus Skill in the Cross Section of Mutual Fund Returns.pdf:pdf},
month = {dec},
title = {{Luck Versus Skill in the Cross Section of Mutual Fund Returns}},
url = {https://papers.ssrn.com/sol3/papers.cfm?abstract{\_}id=1356021},
year = {2009}
}
@misc{LopezdePrado2017,
author = {{Lopez de Prado}, Marcos},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lopez de Prado - 2017 - The 7 Reasons Most Machine Learning Funds Fail (Presentation Slides).pdf:pdf},
keywords = {Machine learning,backtest overfitting,investment strategies,quantamental investing},
month = {sep},
title = {{The 7 Reasons Most Machine Learning Funds Fail (Presentation Slides)}},
url = {https://papers.ssrn.com/sol3/papers.cfm?abstract{\_}id=3031282},
year = {2017}
}
@article{Harvey2014,
abstract = {We propose a new regression method to select amongst a large group of candidate factors - many of which might be the result of data mining - that purport to explain the cross-section of expected returns. The method is robust to general distributional characteristics of both factor and asset returns. We allow for the possibility of time-series as well as cross-sectional dependence. The technique accommodates a wide range of test statistics such as t-ratios. While our main application focuses on asset pricing, the method can be applied in any situation where regression analysis is used in the presence of multiple testing. This includes, for example, the evaluation of investment manager performance as well as time-series prediction of asset returns.},
author = {Harvey, Campbell R. and Liu, Yan},
doi = {10.2139/ssrn.2528780},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Harvey, Liu - 2014 - Lucky Factors.pdf:pdf},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
keywords = {bootstrap,data mining,factors,fama-macbeth,grs,ization,multiple testing,orthogonal-,predictive regressions,variable selection},
title = {{Lucky Factors}},
url = {http://papers.ssrn.com/abstract=2528780{\%}5Cnhttp://papers.ssrn.com/sol3/papers.cfm?abstract{\_}id=2528780},
year = {2014}
}
@article{Harvey2014a,
abstract = {We provide some new tools to evaluate trading strate- gies. When it is known that many strategies and combinations of strategies have been tried, we need to adjust our evaluation method for these multiple tests. Sharpe ratios and other statistics will be overstated. Our methods are simple to implement and allow for the real-time evaluation of candidate trading strategies.},
author = {Harvey, Campbell R and Liu, Yan},
doi = {10.3905/jpm.2014.40.5.108},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Harvey, Liu - 2014 - Evaluating Trading Strategies.pdf:pdf},
issn = {0095-4918},
journal = {The Journal of Portfolio Management},
number = {5},
pages = {108--118},
title = {{Evaluating Trading Strategies}},
url = {http://www.iijournals.com/doi/abs/10.3905/jpm.2014.40.5.108},
volume = {40},
year = {2014}
}
@article{Harvey2015,
abstract = {When evaluating a trading strategy, it is routine to discount the Sharpe ratio from a historical backtest. The reason is simple: there is inevitable data min- ing by both the researcher and by other researchers in the past. Our paper provides a statistical framework that systematically accounts for these multiple tests. We propose a method to determine the appropriate haircut for any given reported Sharpe ratio. We also provide a profit hurdle that any strategy needs to achieve in order to be deemed “significant”.},
author = {Harvey, Campbell R and Liu, Yan},
doi = {10.2139/ssrn.2345489},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Harvey, Liu - 2015 - Backtesting.pdf:pdf},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
keywords = {Backtest,Data mining,Data snooping,Haircut,Minimum profit hurdle.,Multiple tests,Out-of-sample tests,Overfitting,Sharpe ratio,Trad- ing strategies,VaR,Value at Risk},
title = {{Backtesting}},
url = {http://www.ssrn.com/abstract=2345489},
year = {2015}
}
@article{Patton2008,
author = {Patton, Andrew and Timmermann, Allan and Patton, Ndrew},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Patton, Timmermann, Patton - 2008 - Portfolio Sorts and Tests of Cross-Sectional Patterns in Expected Returns.pdf:pdf},
title = {{Portfolio Sorts and Tests of Cross-Sectional Patterns in Expected Returns}},
url = {http://public.econ.duke.edu/{~}ap172/Patton{\_}sorts{\_}pres{\_}jun08.pdf},
year = {2008}
}
@article{Dewald1986,
abstract = {This paper examines the role of replication in empirical economic research. It presents the findings of a two-year study that collected programs and data from authors and attempted to replicate their published results. Our research provides new and important information about the extent and causes of failures to replicate published results in economics. Our findings suggest that inadvertent errors in published empirical articles are a commonplace rather thana rare occurrence. CR - Copyright {\&}{\#}169; 1986 American Economic Association},
author = {Dewald, William G and Thursby, Jerry G. and Anderson, Richard G.},
doi = {10.2307/1806061},
isbn = {0002-8282},
issn = {00028282},
journal = {The American Economic Review},
number = {4},
pages = {587--603},
publisher = {American Economic Association},
title = {{Replication in Empirical Economics: The Journal of Money, Credit and Banking Project}},
url = {https://econpapers.repec.org/article/aeaaecrev/v{\_}3a76{\_}3ay{\_}3a1986{\_}3ai{\_}3a4{\_}3ap{\_}3a587-603.htm http://www.jstor.org/stable/1806061},
volume = {76},
year = {1986}
}
@article{Brodeur2016,
author = {Brodeur, Abel and L{\'{e}}, Mathias and Sangnier, Marc and Zylberberg, Yanos},
doi = {10.1257/app.20150044},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brodeur et al. - 2016 - Star Wars The Empirics Strike Back.pdf:pdf},
issn = {1945-7782},
journal = {American Economic Journal: Applied Economics},
month = {jan},
number = {1},
pages = {1--32},
title = {{Star Wars: The Empirics Strike Back}},
url = {http://pubs.aeaweb.org/doi/10.1257/app.20150044},
volume = {8},
year = {2016}
}
@article{Ioannidis2005,
author = {Ioannidis, John P. A.},
doi = {10.1371/journal.pmed.0020124},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf:pdf},
issn = {1549-1676},
journal = {PLoS Medicine},
month = {aug},
number = {8},
pages = {e124},
publisher = {Public Library of Science},
title = {{Why Most Published Research Findings Are False}},
url = {http://dx.plos.org/10.1371/journal.pmed.0020124},
volume = {2},
year = {2005}
}
@article{Berk1999,
author = {Berk, Jonathan B. and Green, Richard C. and Naik, Vasant},
doi = {10.1111/0022-1082.00161},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Berk, Green, Naik - 1999 - Optimal Investment, Growth Options, and Security Returns.pdf:pdf},
issn = {00221082},
journal = {The Journal of Finance},
month = {oct},
number = {5},
pages = {1553--1607},
publisher = {Blackwell Publishers, Inc.},
title = {{Optimal Investment, Growth Options, and Security Returns}},
url = {http://doi.wiley.com/10.1111/0022-1082.00161},
volume = {54},
year = {1999}
}
@misc{Zhang2005,
abstract = {The value anomaly arises naturally in the neoclassical framework with rational ex- pectations. Costly reversibility and countercyclicalprice of risk cause assets in place to be harder to reduce, and hence are riskier than growth options especially in bad times when the price of risk is high. By linking risk and expected returns to economic primitives, such as tastes and technology, my model generates many empirical regularities in the cross-section of returns; it also yields an array of new refutable hypotheses providingfresh directionsforfuture empiricalresearch.},
author = {Zhang, Lu},
booktitle = {Journal of Finance},
doi = {10.1111/j.1540-6261.2005.00725.x},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang - 2005 - The value premium.pdf:pdf},
isbn = {00221082},
issn = {00221082},
month = {feb},
number = {1},
pages = {67--103},
publisher = {Blackwell Publishing, Inc.},
title = {{The value premium}},
url = {http://doi.wiley.com/10.1111/j.1540-6261.2005.00725.x},
volume = {60},
year = {2005}
}
@article{Chriss2005,
abstract = {Modern portfolio theory produces an optimal portfolio from estimates of expected returns and a covariance matrix. We present a method for portfolio optimization based on replacing expected returns with ordering information, that is, with information about the order of the expected returns. We give a simple and economically rational definition of optimal portfolios that extends Markowitz' meanvariance optimality condition in a natural way; in particular, our construction allows full use of covariance information. We also provide efficient numerical algorithms. The formulation we develop is very general and is easily extended to a variety of cases, for example, where assets are divided into multiple sectors or there are multiple sorting criteria available.},
author = {Chriss, Neil and Almgren, Robert},
doi = {10.2139/ssrn.720041},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chriss, Almgren - 2005 - Portfolios from sorts.pdf:pdf},
issn = {1556-5068},
journal = {Available at SSRN 720041},
pages = {1--22},
title = {{Portfolios from sorts}},
url = {https://cims.nyu.edu/{~}almgren/papers/sort.pdf http://papers.ssrn.com/sol3/papers.cfm?abstract{\_}id=720041},
year = {2005}
}
@article{Hou2015,
abstract = {This paper compares the Hou, Xue, and Zhang (2015) q-factor model and the Fama and French (2015a) five-factor model on both conceptual and empirical grounds. Four concerns cast doubt on the five-factor model: The internal rate of return often correlates negatively with the one-period-ahead expected return; the value factor seems redundant in the data; the expected investment tends to correlate positively with the one-period-ahead expected return; and past investment is a poor proxy for the expected investment. Empirically, the four-factor q-model outperforms the five-factor model, especially in capturing price and earnings momentum and profitability anomalies.},
author = {Hou, Kewei and Xue, Chen and Zhang, Lu and Cooper, Ilan and Giovinazzo, Raife and Kozak, Serhiy and Murray, Scott and Opp, Christian and Shanken, Jay and Simin, Timothy and Wang, Zhenyu and Berk, Jonathan and Brennan, Michael and Chapman, David and Kolari, Jim and Li, Dongxu and Poterba, Jim and Sensoy, Berk and Stulz, Ren{\'{e}} and Titman, Sheridan and Weisbach, Michael and Werner, Ingrid},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hou et al. - 2015 - A Comparison of New Factor Models.pdf:pdf},
title = {{A Comparison of New Factor Models}},
url = {http://fbe.usc.edu/seminars/papers/F{\_}4-22-16{\_}ZHANG.pdf},
year = {2015}
}
@article{Hou2016,
abstract = {This paper conducts a gigantic replication study of asset pricing anomalies by compiling an extensive data library with 437 variables. After microcaps are controlled for, 276 anomalies with NYSE breakpoints and value-weighted returns, as well as 221 with all-but-micro breakpoints and equal-weighted returns, including a vast majority of liquidity variables, are insignificant at the 5{\%} level. When explaining the remaining hundreds of significant anomalies, the q-factor model and a closely related five-factor model are the two best performing models among a long list of models. Investment and profitability are the dominating driving forces in the broad cross section of average stock returns.},
author = {Hou, Kewei and Xue, Chen and Zhang, Lu and Cooper, Ilan and Giovinazzo, Raife and Kozak, Serhiy and Murray, Scott and Ng, David and Opp, Christian and Shanken, Jay and Simin, Timothy and Wang, Zhenyu and Berk, Jonathan and Brennan, Michael and Chapman, David and Keim, Don and Kolari, Jim and Li, Dongxu and Poterba, Jim and Sensoy, Berk and Stambaugh, Rob and Stulz, Ren{\'{e}} and Titman, Sheridan and Weisbach, Michael and Werner, Ingrid and Yao, Tong and Yaron, Amir},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hou et al. - 2016 - A Comparison of New Factor Models.pdf:pdf},
title = {{A Comparison of New Factor Models}},
url = {http://theinvestmentcapm.com/Comparison2016November.pdf},
year = {2016}
}
@article{Carlson2004,
abstract = {We show that corporate investment decisions can explain the conditional dynamics in expected asset returns. Our approach is similar in spirit to Berk, Green, and Naik (1999), but we introduce to the investment problem operating leverage, reversible real options, fixed adjustment costs, and finite growth opportunities. Asset betas vary over time with historical investment decisions and the current product market demand. Book-to-market effects emerge and relate to operating leverage, while size captures the residual importance of growth options relative to assets in place. We estimate and test the model using simulation methods and reproduce portfolio excess returns comparable to the data. CORPORATE INVESTMENT DECISIONS are often evaluated in a real options context,},
author = {Carlson, Murray and Fisher, Adlai and Giammarino, Ron},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Carlson, Fisher, Giammarino - 2004 - Corporate Investment and Asset Price Dynamics Implications for the Cross-section of Returns.pdf:pdf},
journal = {THE JOURNAL OF FINANCE},
number = {6},
title = {{Corporate Investment and Asset Price Dynamics: Implications for the Cross-section of Returns}},
url = {http://faculty.haas.berkeley.edu/garleanu/ReadingGroup{\_}AP{\_}Sp2012/CarlsonFisherGiammarino2004.pdf},
year = {2004}
}
@inproceedings{Stodden2013,
abstract = {Science is built upon foundations of theory and experiment validated and improved through open, trans-parent communication. With the increasingly central role of computation in scientific discovery this means communicating all details of the computations needed for others to replicate the experiment, i.e. making avail-able to others the associated data and code. The " reproducible research " movement recognizes that traditional scientific research and publication practices now fall short of this ideal, and encourages all those involved in the production of computational science – scientists who use computational methods and the institutions that employ them, journals and dissemination mechanisms, and funding agencies – to facilitate and practice really reproducible research. This report summarizes discussions that took place during the ICERM Workshop on Reproducibility in Computational and Experimental Mathematics, held December 10-14, 2012. The main recommendations that emerged from the workshop discussions are: 1. It is important to promote a culture change that will integrate computational reproducibility into the research process. 2. Journals, funding agencies, and employers should support this culture change. 3. Reproducible research practices and the use of appropriate tools should be taught as standard operat-ing procedure in relation to computational aspects of research. The workshop discussions included presentations of a number of the diverse and rapidly growing set of soft-ware tools available to aid in this effort. We call for a broad implementation of these three recommendations across the computational sciences.},
author = {Stodden, V and Bailey, D H and Borwein, J and Leveque, R J and Rider, W and Stein, W},
booktitle = {ICERM workshop},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stodden et al. - 2013 - Setting the Default to Reproducible Reproducibility in Computational and Experimental Mathematics.pdf:pdf},
pages = {19},
title = {{Setting the Default to Reproducible Reproducibility in Computational and Experimental Mathematics}},
url = {http://www.davidhbailey.com/dhbpapers/icerm-report.pdf},
year = {2013}
}
@article{Bloomberg2007,
author = {Bloomberg},
doi = {10.1007/b138723},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bloomberg - 2007 - Function Reference.pdf:pdf},
isbn = {3-211-21137-3},
journal = {Security},
number = {October},
title = {{Function Reference}},
year = {2007}
}
@article{Villalonga2000,
abstract = {This paper documents the process followed to match establishment-level data from the Census Bureau's Business Information Tracking Series (BITS) to firm-level data from Standard and Poor's Compustat. The resulting database provides information on the financial characteristics of public U.S. firms and a more objective and detailed breakdown of their activities by industry than that offered by segment-level data. The merged database is therefore an extremely rich source of information that can be used to investigate a variety of topics. The paper also compares the different large-sample databases that have so far been available for academic research within firms, with a particular application to research on corporate diversification. Finally, a brief description of the matching file that is now available at the Center for Economic Studies is also provided.},
author = {Villalonga, Bel{\'{e}}n},
keywords = {Compustat,The Business Information Tracking Series (BITS),diversification,the Longitudinal Enterprise and Establishment Micr},
number = {December},
title = {{Matching BITS to Compustat: Towards richer data for large sample research within firms}},
year = {2000}
}
@article{Harvey2013,
abstract = {Hundreds of papers and factors attempt to explain the cross-section of expected returns. Given this extensive data mining, it does not make sense to use the usual criteria for establishing significance. Which hurdle should be used for current research? Our paper introduces a new multiple testing framework and provides historical cutoffs from the first empirical tests in 1967 to today. A new factor needs to clear a much higher hurdle, with a t-statistic greater than 3.0. We argue that most claimed research findings in financial economics are likely false.Received October 22, 2014; accepted June 15, 2015 by Editor Andrew Karolyi.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Harvey, Campbell R. and Liu, Yan and Zhu, Heqing},
doi = {10.1093/rfs/hhv059},
eprint = {9809069v1},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Harvey, Liu, Zhu - 2013 - ...and the Cross-Section of Expected Returns.pdf:pdf},
isbn = {9780874216561},
issn = {14657368},
journal = {Review of Financial Studies},
keywords = {3-factor model,beta,bonferroni,hml,idiosyncratic volatility,liquidity,momentum,multiple tests,risk factors,skewness,smb,volatility},
number = {1},
pages = {5--68},
pmid = {15003161},
primaryClass = {arXiv:gr-qc},
title = {{...and the Cross-Section of Expected Returns}},
volume = {29},
year = {2013}
}
@article{Gebbie2014,
author = {Gebbie, Tim},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gebbie - 2014 - Evidence of characteristic cross-sectional pricing on the JSE.pdf:pdf},
title = {{Evidence of characteristic cross-sectional pricing on the JSE}},
year = {2014}
}
@article{Daniel1997a,
abstract = {Firm sizes and book-to-market ratios are both highly correlated with the average returns of common stocks. Fama and French (1993) argue that the association between these characteristics and returns arise because the characteristics are proxies for nondiversifiable factor risk. In contrast, the evidence in this article indicates that the return premia on small capitalization and high book-to-market stocks does not arise because of the comovements of these stocks with pervasive factors. It is the characteristics rather than the covariance structure of returns that appear to explain the cross-sectional variation in stock returns.},
author = {Daniel, Kent and Titman, Sheridan},
doi = {10.2307/2329554},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Daniel, Titman - 1997 - Evidence on the Characteristics of Cross Sectional Variation in Stock Returns.pdf:pdf},
isbn = {00221082},
issn = {00221082},
journal = {The Journal of Finance},
number = {1},
pages = {1},
pmid = {2329554},
title = {{Evidence on the Characteristics of Cross Sectional Variation in Stock Returns}},
url = {http://www.jstor.org/stable/2329554?origin=crossref},
volume = {52},
year = {1997}
}
@article{Hou2017,
abstract = {The anomalies literature is infested with widespread p-hacking. We replicate the entire anomalies literature in finance and accounting by compiling a largest-to-date data library that contains 447 anomaly variables. With microcaps alleviated via New York Stock Exchange breakpoints and value-weighted returns, 286 anomalies (64{\%}) including 95 out of 102 liquidity variables (93{\%}) are insignificant at the conventional 5{\%} level. Imposing the cutoff t-value of three raises the number of insignificance to 380 (85{\%}). Even for the 161 significant anomalies, their magnitudes are often much lower than originally reported. Out of the 161, the q-factor model leaves 115 alphas insignificant (150 with t {\textless} 3). In all, capital markets are more efficient than previously recognized.},
author = {Hou, Kewei and Xue, Chen and Zhang, Lu},
doi = {10.2139/ssrn.2190976},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hou, Xue, Zhang - 2017 - Replicating anomalies.pdf:pdf},
issn = {1556-5068},
journal = {NBER Working Papers},
keywords = {Asset Pricing,Corporate Finance,Economic Fluctua},
number = {No. 23394},
title = {{Replicating anomalies}},
url = {https://papers.ssrn.com/sol3/papers.cfm?abstract{\_}id=2961979},
year = {2017}
}
@article{Wilcox2015,
abstract = {We discuss the finding that cross-sectional characteristic based models have yielded portfolios with higher excess monthly returns but lower risk than their arbitrage pricing theory counterparts in an analysis of equity returns of stocks listed on the JSE. Under the assumption of general no-arbitrage conditions, we argue that evidence in favour of characteristic based pricing implies that information is more likely assimilated by means of nonlinear pricing kernels for the markets considered.},
archivePrefix = {arXiv},
arxivId = {1310.4067},
author = {Wilcox, Diane L. and Gebbie, Tim J.},
doi = {10.1080/10293523.2014.994437},
eprint = {1310.4067},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wilcox, Gebbie - 2015 - On pricing kernels, information and risk.pdf:pdf;:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wilcox, Gebbie - 2015 - On pricing kernels, information and risk(2).pdf:pdf},
issn = {20770227},
journal = {Investment Analysts Journal},
keywords = {Arbitrage pricing theory,Characteristic-based models,Linear pricing kernel,Non-linear pricing kernel,Size effect,Value effect},
number = {1},
pages = {1--19},
title = {{On pricing kernels, information and risk}},
volume = {44},
year = {2015}
}
@article{Bailey2014,
abstract = {We prove that high simulated performance is easily achievable after backtesting a relatively small number of alternative strategy configurations, a practice we denote “backtest overfitting”. The higher the number of configurations tried, the greater is the probability that the backtest is overfit. Because most financial analysts and academics rarely report the number of configurations tried for a given backtest, investors cannot evaluate the degree of overfitting in most investment proposals. The implication is that investors can be easily misled into allocating capital to strategies that appear to be mathematically sound and empirically supported by an outstanding backtest. Under memory effects, backtest overfitting leads to negative expected returns out-of-sample, rather than zero performance. This may be one of several reasons why so many quantitative funds appear to fail.},
author = {Bailey, David H. and Borwein, Jonathan M. and {L{\'{o}}pez de Prado}, Marcos and Zhu, Qiji Jim},
doi = {10.2139/ssrn.2308659},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bailey et al. - 2014 - Pseudo-Mathematics and Financial Charlatanism The Effects of Backtest Overfitting on Out-of-Sample Performance.pdf:pdf;:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bailey et al. - 2014 - Pseudo-Mathematics and Financial Charlatanism The Effects of Backtest Overfitting on Out-of-Sample Performance(2).pdf:pdf;:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bailey et al. - 2014 - Pseudo-Mathematics and Financial Charlatanism The Effects of Backtest Overfitting on Out-of-Sample Performance(3).pdf:pdf},
issn = {1556-5068},
journal = {Notices of the AMS},
keywords = {Sharpe ratio,backtest,historical simulation,investment strategy,minimum backtest length,optimization,performance degradation,probability of backtest over-fitting},
number = {5},
pages = {458--471},
title = {{Pseudo-Mathematics and Financial Charlatanism: The Effects of Backtest Overfitting on Out-of-Sample Performance}},
url = {http://papers.ssrn.com/abstract=2308659},
volume = {61},
year = {2014}
}
@book{Martin2009,
address = {Boston},
author = {Martin, Robert C},
edition = {First},
file = {:home/riaz/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Martin - 2009 - Clean Code A Handbook of Agile Software Craftsmanship.pdf:pdf},
isbn = {9780132350884},
publisher = {Prentice Hall},
title = {{Clean Code: A Handbook of Agile Software Craftsmanship}},
year = {2009}
}
